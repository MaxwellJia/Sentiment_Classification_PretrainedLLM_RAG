import os
import sys
import pickle
from query import (
    get_doc_to_norm,
    run_query,
)
from indexer import (
    make_doc_ids,
    get_token_list,
    index_from_tokens,
)
from ctransformers import AutoModelForCausalLM


def build_index():
    """build index for the documents."""
    print('Build index ...')
    # get a list of documents
    doc_path = os.path.join("data", "gov", "documents")
    doc_list = [os.path.join(dpath, f) for (dpath, dnames, fnames) in os.walk(doc_path) for f in fnames \
        if not f.startswith('.')]  # excludes UNIX hidden files such as ".DS_Store" generated by MacOS
    num_docs = len(doc_list)
    print(f"Found {num_docs} documents.")

    # assign unique doc_ids to each of the documents
    doc_ids = make_doc_ids(doc_list)
    # get the list of tokens in all the documents
    tok_list = get_token_list(doc_list, doc_ids)

    # build the index from the list of tokens
    index, doc_freq = index_from_tokens(tok_list)
    del tok_list # free some memory

    # compute doc norms (in practice we would want to store this on disk, for
    # simplicity in this assignment it is computed here)
    doc_norms = get_doc_to_norm(index, doc_freq, num_docs) 

    # get a reverse mapping from doc_ids to document paths
    id_to_doc = {docid: path for (path, docid) in doc_ids.items()}

    # store the index to disk
    pickle.dump((index, doc_freq, doc_norms, id_to_doc, num_docs), open("index_data.pkl", "wb"))


def get_top_docs(query, topk=3):
    """query and return the k top ranked documents.

    Args:
        query (str): the query string
        topk (int): the number of top ranked documents to return

    Returns:
        list(str): the list of k top ranked document file names
    """
    # load the index from disk
    index_file = "index_data.pkl"
    if not os.path.exists(index_file):
        build_index()
    (index, doc_freq, doc_norms, id_to_doc, num_docs) = pickle.load(open(index_file, "rb"))
    res = run_query(query, index, doc_freq, doc_norms, num_docs)
    print(f'\nQuery: {query}')
    print(f'Top-{topk} documents (similarity scores):')
    file_paths = []
    for (docid, sim) in res[:topk]:
        fpath = id_to_doc[docid]
        file_paths.append(fpath)
        print(f'{fpath} {sim:.4f}')
    return file_paths


def load_llm():
    """load a pre-trained LLM"""
    model_repo = "TheBloke/Llama-2-7b-Chat-GGUF"  # name of a Hugging Face Hub model repo
    model_file = "llama-2-7b-chat.Q4_K_M.gguf"   # name of the model file in repo
    print(f'loading model "{model_repo}/{model_file}" ...')
    # to work offline, you may set `local_files_only` to True once the model files have been downloaded.
    llm = AutoModelForCausalLM.from_pretrained(model_repo, model_file=model_file, local_files_only=False)
    return llm


def query_llm(llm, prompt, max_new_tokens=256, verbose=True, seed=101, **kwargs):
    """use a pre-trained LLM to generate (and stream) text w.r.t. the given prompt.

    Args:
        llm (LLM): a pre-trained LLM
        prompt (str): the prompt for text generation
        max_new_tokens (int): the maximum number of new tokens may be generated by the LLM
        verbose (boolean): print additional details if True
        seed (int): the seed value to use for sampling tokens

    Returns:
        str: the generated (and streamed) text
    """
    assert len(prompt.strip()) > 0
    kwargs.update({
        'max_new_tokens': max_new_tokens,
        'seed': seed,
        'stream': True,  # whether to stream the generated text
        'reset': True,  # whether to reset the model state before generating text
    })
    
    if verbose:
        sys.stdout.write(f'\n{prompt}')

    ans = []
    for text in llm(prompt, **kwargs):
        print(text, end="", flush=True)
        ans.append(text)
    sys.stdout.write('\n')
    return "".join(ans)


def get_prompt(query, context=None, num_word=50):
    """create a prompt for the given query.

    Args:
        query (str): the query string
        context (str): optional, the context string
        num_word (int): the (approximate) number of words in the generated text

    Returns:
        str: the prompt string
    """
    if context is None:
        return f'Answer the question below in text using about {num_word} words, your answer should be in bullet points.\n\nQuestion:\n{query}\n\nAnswer:\n'
    
    # TODO: create a prompt for the given query and context
    else:
        prompt = f'Given the following context, answer the question below in text using about {num_word} words.Your answer should be in bullet points.\n\nContext:\n{context}\n\nQuestion:\n{query}\n\nAnswer:\n'

    return prompt


def summarise_text(llm, text, num_word=50, seed=101):
    """summarise text using the LLM.

    Args:
        llm (LLM): a pre-trained LLM
        text (str): the text to be summarised
        num_word (int): the (approximate) number of words in the generated summary
        seed (int): the seed value to use for sampling tokens in the LLM

    Returns:
        str: the summary
    """

    # TODO: summarise the given text using a pre-trained LLM
    # you may need to truncate the text so that the prompt and generated summary fit in the context window
    # of the LLM (otherwise it may cause an error "Number of tokens exceeded maximum context length")

    # you may use a different prompt for the summarisation task
    prompt = f"Summarise the below text using about {num_word} words.\n---\n{text}\n---\nSummary:\n"

    # truncate text to fit within limits
    text = " ".join(text.split()[:256])
    prompt = f"Summarise the text below using about {num_word} words.\n\n---\n{text}\n---\n\nSummary:\n"

    summary = query_llm(llm=llm, prompt=prompt, max_new_tokens=num_word, verbose=False, seed=seed)

    return summary


def retrieval_augmented_generation(llm, query, num_top_doc=3, num_word=50, verbose=True, seed=101):
    """generate text using an LLM through Retrieval-Augmented Generation (RAG) for the query.

    Args:
        llm (LLM): a pre-trained LLM
        query (str): the query string
        num_top_doc (int): the number of top ranked documents to be used in RAG
        num_word (int): the (approximate) number of words in the generated text
        verbose (boolean): print additional details if True
        seed (int): the seed value to use for sampling tokens in the LLM

    Returns:
        str: the generated text
    """

    # TODO: implement this function, it should
    # 1) retrieve the top `num_top_doc` ranked documents for `query`
    #    using the `get_top_docs` function;
    # 2) summarise each of the top ranked documents using the `summarise_text` function,
    #    and create a context string e.g. by concatenating the summaries;
    # 3) create an prompt that incorporates the query and the context;
    # 4) generate text using a pre-trained LLM and the prompt, return the generated text.

    top_docs = get_top_docs(query, topk=num_top_doc)

    context = ""
    for doc_path in top_docs:
        with open(doc_path, 'r', encoding='utf-8', errors='ignore') as f:
            doc_text = f.read()
        summary_text = summarise_text(llm=llm, text=doc_text, num_word=num_word, seed=seed)
        context += f"Document: {doc_path}\nSummary: {summary_text}\n\n"

    prompt = get_prompt(query=query, context=context, num_word=num_word)

    ans = query_llm(llm=llm, prompt=prompt, max_new_tokens=num_word, verbose=verbose, seed=seed)

    return ans


if __name__ == '__main__':
    llm = load_llm()
    for query in [
        'Is nuclear power plant eco-friendly?',
        'How to stay safe during severe weather?',
    ]: 
        # uncomment to directly query the pre-trained LLM
        # query_llm(llm, get_prompt(query), verbose=True, seed=101)

        # uncomment to query the pre-trained LLM through RAG
        retrieval_augmented_generation(llm, query, num_top_doc=3, num_word=50, verbose=True, seed=101)

